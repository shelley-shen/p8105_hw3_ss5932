---
title: "Homework 3"
author: "Shelley Shen"
date: "10/6/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(dplyr)
library(rnoaa)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6, 
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

Load Instacart data. 

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, oder ID, order day, and order hour. There are also item variables == name, aisle, department, and some numeric codes. 



How many aisles, and which are most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```


Let's make a plot of the number of items ordered in each aisle, limited to those aisles with >10,000 items ordered. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table of most popular items in each aisle of interest: baking ingredients, dog food, packaged vegetables fruits.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


Make table for mean hour of day when Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```



## Problem 2

**Part 1**
- tidy using pivot long --> columns are week, day of week, minute of day, and activity count. 
- mutate for weekday vs. weekend

Load in the accelerometer data and tidy. 

```{r}
accel_df = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  group_by(day) %>% 
  mutate(
    minute = as.numeric(minute), 
    day = as.factor(day),
    activity_count = as.numeric(activity_count),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
    weekday_vs_weekend = if_else(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))
```

The accelerometer dataset contains information regarding the amount of physical activity measured in an accelerometer worn by a 63 year-old male with BMI 25 and congestive heart failure and is being monitored by the Advanced Cardiac Care Center of Columbia University Medical Center. After tidying and organizing the data, the resulting dataset has `r nrow(accel_df)` observations with `r ncol(accel_df)` columns and `r nrow(accel_df)` rows. The variables in the data include the week, day of the week, day of the trial, whether it was the weekday or weekend, minute of the day, and activity count. 

**Part 2**

Find the aggregate activity level for each day and display in a table. 

```{r}
accel_df %>% 
  group_by(day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  knitr::kable(digits = 1)
```

From this table of total activity per day of the week, we observe that on average, Friday has the most activity while Saturday has the least amount of activity. 


**Part 3**

Make a plot depicting the time course for each day. 

```{r}
accel_df %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_line(alpha = 0.3) +
  stat_smooth() +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("00:00", "4:00", "8:00", "12:00", "16:00", "20:00", "23:59")
  ) +
  labs(
    title = "24 hour activity level by day of the week",
    x = "Time of day (hour)",
    y = "Activity level")
```

From this graph of the subject's activity levels collected by the accelerometer, we can see that much of his activity levels occurred during the day between the waking hours of 7:00am (7:00) and 10:00pm (22:00). There were spurts of activity at certain times on specific days. For example, there are peaks of activity late Sunday mornings between 10:00am and 12:00pm, and Friday evenings between 8:00pm and 10:00pm appear particularly active. There is significantly less activity during the late night and early morning hours, as we can presume the subject was sleeping. 


## Problem 3

Load the NY NOAA dataset from the P8105 library. 

```{r}
data("ny_noaa")
```

The NY NOAA dataset is from the National Oceanic and Atmospheric Association (NOAA) National Cliamtic Data Center containing information on weather data collected from all the weather stations in New York from January 1, 1981 through December 31, 2010. The dataset contains the variables weather station ID, observation date, amount of precipitation, snowfall, snow depth, maximum and minimum temperature. There are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 


**Part 1**
- separate date variable into month, day, year
- make sure unit is reasonable based on data dictionary
- need to count and rank

Separate the date variable into month, day, year; convert units to whole units

```{r}
ny_noaa_df = 
  ny_noaa %>%
  drop_na() %>% 
  separate(date, c("year", "month", "day"), convert = TRUE) %>% 
  mutate(
    year = as.integer(year),
    month = month.name[as.integer(month)],
    day = as.integer(day),
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
    prcp = as.numeric(prcp) / 10
  )
```

Find the most commonly observed value for snowfall. 

```{r}
ny_noaa_df %>% 
  count(snow, name = "snow_obs") %>% 
  arrange(desc(snow_obs))
```

The most commonly observed value for snowfall is 0 centimeters with 2,008,508 observations for this value. This can be explained by a lack of snowfall on the majority of days or that there are certain stations which receive less snowfall than others. 


**Part 2**
- need to group so avg max temp separately in Jan and in July --> use group_by and summarize 
- group by station, year, month, then summarize avg max temp, filter
- y = station, x = years 

Create two-panel plot showing the average *max* temperature in January and July in each station across years. 

```{r}
ny_noaa_df %>% 
  group_by(id, year, month) %>% 
  filter(month %in% c("January", "July")) %>%
  summarize(avg_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = avg_tmax, color = month)) +
    geom_point() +
    facet_grid(. ~ month) +
    labs(
      title = "Average max temperature (C) over time in January vs. July",
      x = "Year",
      y = "Average max temperatures (C)")
```

Overall, the average maximum temperature was much higher in July than in January with consistent readings across stations. There are a few times when there was a large difference in maximum temperature difference for certain stations.  

**Part 3**
- make 2 plots and merge together =
- 1) contour plot, bin plot, or hex plot
- 2) filter, show distribution (fox plot, violin, ridge -- one ridge per year)

Make a two-panel plot showing tmax and tmin for the entire dataset.

```{r}

```












