---
title: "Homework 3"
author: "Shelley Shen"
date: "10/6/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6, 
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

Load Instacart data. 

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, oder ID, order day, and order hour. There are also item variables == name, aisle, department, and some numeric codes. 



How many aisles, and which are most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```


Let's make a plot of the number of items ordered in each aisle, limited to those aisles with >10,000 items ordered. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table of most popular items in each aisle of interest: baking ingredients, dog food, packaged vegetables fruits.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


Make table for mean hour of day when Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```



## Problem 2

**Part 1**
- tidy using pivot long --> columns are week, day of week, minute of day, and activity count. 
- mutate for weekday vs. weekend

Load in the accelerometer data and tidy. 

```{r}
accel_df = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  group_by(day) %>% 
  mutate(
    minute = as.numeric(minute), 
    day = as.factor(day),
    activity_count = as.numeric(activity_count),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
    weekday = if_else(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))
```

The accelerometer dataset contains information regarding the amount of physical activity measured in an accelerometer worn by a 63 year-old male with BMI 25 and congestive heart failure and is being monitored by the Advanced Cardiac Care Center of Columbia University Medical Center. After tidying and organizing the data, the resulting dataset has `r nrow(accel_df)` observations with `r ncol(accel_df)` columns and `r nrow(accel_df)` rows. The variables in the data include the week, day of the week, whether it was the weekday or weekend, minute of the day, and activity count. 


**Part 2**
- aggregate activity for total day --> group_by week, day and summarize --> 35 days --> week # and day of week
- trends inherent: more activity during certain day, etc.


Find the aggregate activity level for each day and display in a table. 

```{r}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  knitr::kable(digits = 1)
```


**Part 3**
- single panel plot: x = minute, y = activity count --> scatterplot with geom_line, color to indicate day of week, trends

Make a plot depicting the time course for each day. 

```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_count)) +
  geom_point(aes(color = day, alpha = 0.3))
```





Potential issues:
- day of week by default in alphabetical order --> use factors to order how we want
- when pivot_longer, need to check if minute is in numeric format or some other format so the plot will make sense when merged



## Problem 3

```{r}
data("ny_noaa")
```

**Part 1**
- separate date variable into month, day, year
- make sure unit is reasonable based on data dictionary
- need to count and rank

**Part 2**
- need to group so avg max temp separately in Jan and in July --> use group_by and summarize 
- group by station, year, month, then summarize avg max temp, filter
- y = station, x = years 


**Part 3**
- make 2 plots and merge together 
- 1) contour plot, bin plot, or hex plot
- 2) filter, show distribution (fox plot, violin, ridge -- one ridge per year)












